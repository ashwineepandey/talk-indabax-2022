{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "np.set_printoptions(suppress=True) # Removes printing of scientific notation values\n",
    "sns.set_theme()\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem\n",
    "\n",
    "Let us look at a problem that is a simple finite Markov Decision Process. \n",
    "Consider the following 5 x 5 grid:\n",
    "\n",
    "![Gridworld](imgs/Gridworld.png)\n",
    "\n",
    "## Define the problem\n",
    "\n",
    "- The cells of the grid correspond to the the states of the environment. \n",
    "- At each cell, 4 actions are possible: _north_, _south_, _east_, _west_ - which deterministically cause the agent to move 1 cell in the respective direction on the grid.\n",
    "- Actions that would the agent off the grid leave its location unchanges, but also result in a reward of -1.\n",
    "- From state A, any action yields a reward of +10 and takes the agent to A'.\n",
    "- From state B, any action yields a reward of +5 and takes the agent to B'.\n",
    "- All other actions result in a reward of 0.\n",
    "\n",
    "Find the Optimal Value Function using Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Grid:\n",
    "    x_min: int\n",
    "    x_max: int\n",
    "    y_min: int\n",
    "    y_max: int\n",
    "    x_size: int\n",
    "    y_size: int\n",
    "\n",
    "@dataclass\n",
    "class Coordinate:\n",
    "    x: int\n",
    "    y: int\n",
    "\n",
    "@dataclass\n",
    "class Direction:\n",
    "    x: int\n",
    "    y: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_reward(loc: Coordinate, grid: Grid, A, B) -> int:\n",
    "    \"\"\"\n",
    "    This function takes the location of a state and returns the reward for it.\n",
    "\n",
    "    Args:\n",
    "        loc: current location of agent\n",
    "        grid: specifications of grid\n",
    "        A: Position A coordinate\n",
    "        B: Position B coordinate\n",
    "    \n",
    "    Returns:\n",
    "        reward: value assigned to the state based on environment\n",
    "\n",
    "    \"\"\"\n",
    "    if loc == A: #Reward for position A\n",
    "        reward = 10\n",
    "    elif loc == B: #Reward for position B\n",
    "        reward = 5\n",
    "    elif loc.x < grid.x_min or loc.x > grid.x_max or loc.y < grid.y_min or loc.y > grid.y_max: #Negative reward outside grid\n",
    "        reward = -1\n",
    "    else: # Reward for all other states\n",
    "        reward = 0\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_loc(loc: Coordinate, grid: Grid) -> bool:\n",
    "    \"\"\"\n",
    "    This function checks if the coordinate of the location is within the confines of the grid. \n",
    "\n",
    "    Args:\n",
    "        loc: current location of agent\n",
    "        grid: specifications of grid\n",
    "    \n",
    "    Returns: True if loc is valid, else False.\n",
    "    \"\"\"\n",
    "    if loc.x < grid.x_min or loc.x > grid.x_max or loc.y < grid.y_min or loc.y > grid.y_max:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(loc: Coordinate, action: int) -> (tuple, float):\n",
    "    \"\"\"\n",
    "    This function takes the current location of agent and the proposed action and returns the next state it will be in.\n",
    "\n",
    "    Args:\n",
    "        loc: current location of agent\n",
    "        action: direction in which agent shall move.\n",
    "    \n",
    "    Returns:\n",
    "        next_loc: coordinate for next state on grid based on environment constraints\n",
    "        reward: value assigned to the state based on environment\n",
    "\n",
    "    \"\"\"\n",
    "    loc = Coordinate(loc[0], loc[1]) #Convert to Coordinate object for convenience\n",
    "    A = Coordinate(1, 0) # Define special state A\n",
    "    B = Coordinate(3, 0) # Define special state B\n",
    "    Ap = Coordinate(1, 4) # Define special state A prime\n",
    "    Bp = Coordinate(3, 2) # Define special state B prime\n",
    "    reward = _calc_reward(loc, grid, A, B)\n",
    "    if loc == A:\n",
    "        proposed_loc = Ap\n",
    "    elif loc == B:\n",
    "        proposed_loc = Bp\n",
    "    else:\n",
    "        if action == 0:\n",
    "            proposed_loc = Coordinate(loc.x, loc.y - 1)\n",
    "        elif action == 1:\n",
    "            proposed_loc = Coordinate(loc.x, loc.y + 1)\n",
    "        elif action == 2:\n",
    "            proposed_loc = Coordinate(loc.x + 1, loc.y)\n",
    "        elif action == 3:\n",
    "            proposed_loc = Coordinate(loc.x - 1, loc.y)\n",
    "\n",
    "    if is_valid_loc(proposed_loc, grid):\n",
    "        next_loc = proposed_loc\n",
    "    else:\n",
    "        next_loc = loc\n",
    "    \n",
    "    next_loc = tuple((next_loc.x, next_loc.y)) #Convert to tuple object for dict\n",
    "    return next_loc, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_q(grid: Grid, actions: list) -> dict:\n",
    "    \"\"\"\n",
    "    This function takes the grid and set of possible actions and initialises an empty Q table.\n",
    "\n",
    "    Args:\n",
    "        grid: specifications of grid\n",
    "        actions: set of directions in which agent can move.\n",
    "    \n",
    "    Returns:\n",
    "        q: Q Table\n",
    "    \"\"\"\n",
    "    q = {}\n",
    "    for x in range(grid.x_size):\n",
    "        for y in range(grid.y_size):\n",
    "            q[(x, y)] = {}\n",
    "            for action in actions:\n",
    "                q[(x, y)][action] = 0.\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(eta: float, q: dict, state: tuple, actions: list) -> int:\n",
    "    \"\"\"\n",
    "    This function takes in args to return the next action based on the extent of \n",
    "    randomness or q-values one wants to use controlled by the eta parameter.\n",
    "\n",
    "    Args:\n",
    "        eta: specifications of grid\n",
    "        q: Q Table\n",
    "        state: Coordinate of current state\n",
    "        actions: set of directions in which agent can move.\n",
    "    \n",
    "    Returns:\n",
    "        action: N, S, E or W picked as an integer 0, 1, 2 or 3 respectively. \n",
    "    \"\"\"\n",
    "    next_action_choices = [\"random\", \"argmax\"]\n",
    "    choice = np.random.choice(next_action_choices, 1, p=[eta, 1-eta])\n",
    "    if choice == \"random\":\n",
    "        action = np.random.choice(actions, 1)[0]\n",
    "    elif choice == \"argmax\":\n",
    "        action = np.argmax(list(q[state].values()))\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_direction(arrow: int) -> Coordinate:\n",
    "    \"\"\"\n",
    "    Helper function that determines the direction in which to draw an arrow.\n",
    "\n",
    "    Args:\n",
    "        arrow: variable that defines the direction of the arrow based on N, S, E, W.\n",
    "    \n",
    "    Returns:\n",
    "        direction: gives a coordinate in the direction of the arrow\n",
    "    \"\"\"\n",
    "    if arrow == 0:\n",
    "        direction = Direction(0, -1)\n",
    "    elif arrow == 1:\n",
    "        direction = Direction(0, 1)\n",
    "    elif arrow == 2:\n",
    "        direction = Direction(1, 0)\n",
    "    elif arrow == 3:\n",
    "        direction = Direction(-1, 0)\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_star_arr = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_q_arr(q_star) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Helper function that determines the direction of arrows for every state in the environment.\n",
    "\n",
    "    Args:\n",
    "        q_star: optimal policy\n",
    "    \n",
    "    Returns:\n",
    "        q_star_arr: array of q values for each state in the environment.\n",
    "    \"\"\"\n",
    "    q_star_arr = np.zeros((5, 5))\n",
    "    states = list(q_star.keys())\n",
    "    for state in states:\n",
    "        directions = list(q_star[state].keys())\n",
    "        for direction in directions:\n",
    "            q_val = np.round(q_star[state][direction])\n",
    "            q_star_arr[state[1]][state[0]] = q_val\n",
    "    return q_star_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(grid: Grid, actions: list, alpha: float, eta: float, gamma: float, episodes: int, N: int) -> dict:\n",
    "    \"\"\"\n",
    "    This function implements the Q Learning algorithm on the environment based on the hyper-parameters.\n",
    "\n",
    "    Args:\n",
    "        grid: specifications of grid\n",
    "        actions: set of directions in which agent can move.\n",
    "        alpha: \n",
    "        eta:\n",
    "        gamma: \n",
    "        epsiodes: \n",
    "        N: \n",
    "    \n",
    "    Returns:\n",
    "        q: store of q value for every combination of state and action. \n",
    "    \"\"\"\n",
    "    q = initialise_q(grid, actions) # Initialize q_table \n",
    "    states = list(q.keys())\n",
    "    for i in range(episodes):\n",
    "        state = random.choice(states) # Initialise state with random starting point\n",
    "        for j in range(N):\n",
    "            action = choose_action(eta, q, state, actions)\n",
    "            next_state, reward = move(state, action)\n",
    "            next_action = np.argmax(list(q[next_state].values()))\n",
    "            q[state][action] = q[state][action] + alpha * (reward + gamma * (q[next_state][next_action]) - q[state][action])\n",
    "            state = next_state\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gridworld_results(q_star, q_star_arr):\n",
    "    \"\"\"\n",
    "    Helper function that plots the grid with the arrows indicating the optimal policy that has been learned.\n",
    "\n",
    "    Args:\n",
    "        q_star: optimal policy\n",
    "        q_star_arr: direction of arrows associated to the optimal policy.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(15, 7)\n",
    "    fig.suptitle(fr\"Gridworld states and directions, using Q-Learning\")\n",
    "    fig.supxlabel(\"x\")\n",
    "    fig.supylabel(\"y\")\n",
    "    axs[0].set_title(fr\"$q_*$\")\n",
    "    axs[1].set_title(fr\"$\\pi_* directions$\")\n",
    "    sns.heatmap(q_star_arr, cmap='Spectral', annot=True, ax=axs[0])\n",
    "    plt.imshow(q_star_arr, cmap='Spectral')\n",
    "    states = list(q_star.keys())\n",
    "    for state in states:\n",
    "        arrows = np.argwhere(list(q_star[state].values()) == np.amax(list(q_star[state].values()))).flatten().tolist()\n",
    "        for arrow in arrows:\n",
    "            direction = get_xy_direction(arrow)\n",
    "            plt.quiver(state[0], state[1], direction.x, -direction.y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.1\n",
    "alpha = 0.2\n",
    "gamma = 0.9\n",
    "episodes = 1000\n",
    "N = 1000\n",
    "actions = [0, 1, 2, 3] #Define actions: North, South, East, West\n",
    "grid = Grid(x_min=0, x_max=4, y_min=0, y_max=4, x_size=5, y_size=5) #Define a 5 x 5 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_star = q_learning(grid, actions, alpha, eta, gamma, episodes, N)\n",
    "q_star_arr = get_q_arr(q_star)\n",
    "plot_gridworld_results(q_star, q_star_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('mlai_rl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21830a4e2865c102de8123e943392dd51b6a60ca38bb488740fe9bbd3d36e933"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
